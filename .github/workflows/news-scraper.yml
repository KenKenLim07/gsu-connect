name: GSU News Scraper

# Workflow to automatically scrape GSU news
on:
  schedule:
    - cron: '0 */6 * * *'  # Runs every 6 hours
  workflow_dispatch:  # Allows manual triggering
  push:
    branches:
      - main
    paths:
      - 'gsu-connect-backend/**'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: gsu-connect-backend/package-lock.json

      - name: Install dependencies
        working-directory: gsu-connect-backend
        run: npm ci

      - name: Run scraper
        working-directory: gsu-connect-backend
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}
        run: npx tsx src/index.ts

      - name: Handle failure
        if: failure()
        run: |
          echo "::error::Scraper failed. Check the logs for details."
          exit 1

      - name: Notify on failure
        if: failure()
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          SLACK_CHANNEL: alerts
          SLACK_COLOR: danger
          SLACK_TITLE: GSU News Scraper Failed
          SLACK_MESSAGE: 'Scraper job failed. Check the GitHub Actions logs for details.'
          SLACK_FOOTER: 'GSU Connect'

      - name: Notify on success
        if: success()
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          SLACK_CHANNEL: alerts
          SLACK_COLOR: good
          SLACK_TITLE: GSU News Scraper Succeeded
          SLACK_MESSAGE: 'Scraper job completed successfully.'
          SLACK_FOOTER: 'GSU Connect' 